{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b5c38cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T05:01:01.673514Z",
     "iopub.status.busy": "2025-04-26T05:01:01.673161Z",
     "iopub.status.idle": "2025-04-26T05:01:01.677011Z",
     "shell.execute_reply": "2025-04-26T05:01:01.676440Z"
    },
    "papermill": {
     "duration": 0.011314,
     "end_time": "2025-04-26T05:01:01.678061",
     "exception": false,
     "start_time": "2025-04-26T05:01:01.666747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install svgwrite svgpathtools\n",
    "# !pip install cairosvg bitsandbytes\n",
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "# !pip install -q opencv-python scikit-image pillow\n",
    "# !pip install scour cssutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb4f918c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T05:01:01.687694Z",
     "iopub.status.busy": "2025-04-26T05:01:01.687492Z",
     "iopub.status.idle": "2025-04-26T05:01:01.690121Z",
     "shell.execute_reply": "2025-04-26T05:01:01.689514Z"
    },
    "papermill": {
     "duration": 0.008612,
     "end_time": "2025-04-26T05:01:01.691323",
     "exception": false,
     "start_time": "2025-04-26T05:01:01.682711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dc12e5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T05:01:01.700753Z",
     "iopub.status.busy": "2025-04-26T05:01:01.700548Z",
     "iopub.status.idle": "2025-04-26T05:02:48.848439Z",
     "shell.execute_reply": "2025-04-26T05:02:48.847740Z"
    },
    "papermill": {
     "duration": 107.154304,
     "end_time": "2025-04-26T05:02:48.850071",
     "exception": false,
     "start_time": "2025-04-26T05:01:01.695767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# diffvg_path = kagglehub.dataset_download('tomirol/diffvg')\n",
    "# out_path = Path(\"/tmp/diffvg\").resolve()\n",
    "\n",
    "# if not out_path.exists():\n",
    "#     shutil.copytree(Path(diffvg_path) / \"diffvg\", str(out_path))\n",
    "#     output = subprocess.check_output(f\"pip uninstall tensorflow -y && cd {str(out_path)} && python setup.py install\", shell=True, text=True)\n",
    "\n",
    "# sys.path.append(str(out_path / \"dist/diffvg-0.0.1-py3.10-linux-x86_64.egg\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24fb3a3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T05:02:48.860508Z",
     "iopub.status.busy": "2025-04-26T05:02:48.860261Z",
     "iopub.status.idle": "2025-04-26T05:02:49.582207Z",
     "shell.execute_reply": "2025-04-26T05:02:49.581579Z"
    },
    "papermill": {
     "duration": 0.728312,
     "end_time": "2025-04-26T05:02:49.583533",
     "exception": false,
     "start_time": "2025-04-26T05:02:48.855221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "draw_src_path = kagglehub.dataset_download('tomirol/drawsrc')\n",
    "\n",
    "out_path = Path(\"/tmp/drawsrc\")\n",
    "\n",
    "if not out_path.exists():\n",
    "    shutil.copytree(str(draw_src_path), out_path)\n",
    "\n",
    "sys.path.append(str(out_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a29e49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T05:02:49.593135Z",
     "iopub.status.busy": "2025-04-26T05:02:49.592914Z",
     "iopub.status.idle": "2025-04-26T05:02:49.937151Z",
     "shell.execute_reply": "2025-04-26T05:02:49.936533Z"
    },
    "papermill": {
     "duration": 0.350448,
     "end_time": "2025-04-26T05:02:49.938573",
     "exception": false,
     "start_time": "2025-04-26T05:02:49.588125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "primitive_path = kagglehub.dataset_download('tomirol/primitive')\n",
    "\n",
    "out_path = Path(\"/tmp/primitive\")\n",
    "\n",
    "if not out_path.exists():\n",
    "    shutil.copy(Path(primitive_path) / \"primitive\", out_path)\n",
    "    subprocess.check_output(f\"chmod +x {out_path}\", shell=True, text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23940cdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T05:02:49.948258Z",
     "iopub.status.busy": "2025-04-26T05:02:49.948032Z",
     "iopub.status.idle": "2025-04-26T05:02:49.971515Z",
     "shell.execute_reply": "2025-04-26T05:02:49.970922Z"
    },
    "papermill": {
     "duration": 0.029469,
     "end_time": "2025-04-26T05:02:49.972674",
     "exception": false,
     "start_time": "2025-04-26T05:02:49.943205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import sys\n",
    "import site\n",
    "from importlib import invalidate_caches\n",
    "\n",
    "# Refresh sys.path\n",
    "site.main()\n",
    "invalidate_caches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76066386",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T05:02:49.982300Z",
     "iopub.status.busy": "2025-04-26T05:02:49.982065Z",
     "iopub.status.idle": "2025-04-26T05:03:05.181260Z",
     "shell.execute_reply": "2025-04-26T05:03:05.180584Z"
    },
    "papermill": {
     "duration": 15.20567,
     "end_time": "2025-04-26T05:03:05.182786",
     "exception": false,
     "start_time": "2025-04-26T05:02:49.977116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# import os\n",
    "# os.environ[\"TORCH_COMPILE_DISABLE\"] = \"1\"\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from PIL import Image\n",
    "import re\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "import kagglehub\n",
    "import subprocess\n",
    "import scour.scour\n",
    "\n",
    "\n",
    "import os\n",
    "import kornia\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import json\n",
    "import cairosvg\n",
    "from pathlib import Path\n",
    "import io\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import pydiffvg\n",
    "\n",
    "\n",
    "from src.score_original import VQAEvaluator, ImageProcessor, AestheticEvaluator\n",
    "from src.score_gradient import (\n",
    "    score_original,\n",
    "    score_gradient,\n",
    "    aesthetic_score_original,\n",
    "    vqa_score_original,\n",
    "    aesthetic_score_gradient,\n",
    "    vqa_score_gradient,\n",
    ")\n",
    "from src.preprocessing import apply_preprocessing_torch\n",
    "\n",
    "from src.text_to_svg import text_to_svg\n",
    "\n",
    "\n",
    "device_0 = f\"cuda:{max(0, torch.cuda.device_count() - 2)}\"\n",
    "device_1 = f\"cuda:{max(0, torch.cuda.device_count() - 1)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37ba38bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T05:03:05.193666Z",
     "iopub.status.busy": "2025-04-26T05:03:05.193196Z",
     "iopub.status.idle": "2025-04-26T05:03:06.017796Z",
     "shell.execute_reply": "2025-04-26T05:03:06.017152Z"
    },
    "papermill": {
     "duration": 0.831377,
     "end_time": "2025-04-26T05:03:06.019208",
     "exception": false,
     "start_time": "2025-04-26T05:03:05.187831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class BaseConfig(BaseModel):\n",
    "    width: int = 384\n",
    "    height: int = 384\n",
    "\n",
    "class SDConfig(BaseConfig):\n",
    "    stable_diffusion_path: str = kagglehub.model_download(\"stabilityai/stable-diffusion-v2/pytorch/1/1\")\n",
    "    \n",
    "    # prompt_prefixes: list[str] = [\"Simple, classic image of\"]\n",
    "    # prompt_suffixes: list[str] = [\"with flat color blocks, beautiful, minimal details, solid colors only\"]\n",
    "    # negative_prompts: list[str] = [\"lines, framing, hatching, background, textures, patterns, details, outlines\"]\n",
    "\n",
    "    prompt_prefixes: list[str] = [\"Simple, classic image of\"]\n",
    "    prompt_suffixes: list[str] = [\"beautiful, minimal details, avoid other colors\"]\n",
    "    negative_prompts: list[str] = [\"other colors, detailed\"]\n",
    "\n",
    "    num_inference_steps: int = 25\n",
    "    guidance_scale: int = 20\n",
    "\n",
    "\n",
    "class TextConfig(BaseConfig):\n",
    "    x_position_frac: float = 0.9\n",
    "    y_position_frac: float = 0.9\n",
    "    font_size: int = 45\n",
    "    color: tuple[int, int, int] = (255, 255, 255)\n",
    "    font_path: str = \"\"\n",
    "\n",
    "\n",
    "class PrimitiveConfig(BaseConfig):\n",
    "    mode: int = 8\n",
    "    num_shapes: int = 100\n",
    "\n",
    "\n",
    "class DiffvgConfig(BaseConfig):\n",
    "    num_iterations: int = 40\n",
    "    validation_steps: int = 20\n",
    "    # base_svg_path: str = str(Path(kagglehub.dataset_download('tomirol/aestsvg')) / \"default_aest_0500.svg\")\n",
    "    base_svg_path: str = str(Path(kagglehub.dataset_download('tomirol/aestsvg')) / \"output_aest_650.svg\")\n",
    "    # base_svg_path: str = str(Path(kagglehub.dataset_download('tomirol/aestsvg')) / \"output_aest_144_0.702.svg\")\n",
    "    # base_svg_path: str = str(Path(kagglehub.dataset_download('tomirol/aestsvg')) / \"output_aest_48_0.593.svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7076ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Copyright (c) XiMing Xing. All rights reserved.\n",
    "# Author: XiMing Xing\n",
    "# Description:\n",
    "\n",
    "from typing import Callable, List, Optional, Union, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import custom_bwd, custom_fwd\n",
    "from torchvision import transforms\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionPipeline\n",
    "from diffusers.schedulers.scheduling_pndm import PNDMScheduler\n",
    "\n",
    "\n",
    "class LSDSPipeline(StableDiffusionPipeline):\n",
    "    r\"\"\"\n",
    "    Pipeline for text-to-image generation using Stable Diffusion.\n",
    "    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n",
    "    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n",
    "    Args:\n",
    "        vae ([`AutoencoderKL`]):\n",
    "            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n",
    "        text_encoder ([`CLIPTextModel`]):\n",
    "            Frozen text-encoder. Stable Diffusion uses the text portion of\n",
    "            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n",
    "            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n",
    "        tokenizer (`CLIPTokenizer`):\n",
    "            Tokenizer of class\n",
    "            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n",
    "        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n",
    "        scheduler ([`SchedulerMixin`]):\n",
    "            A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n",
    "            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n",
    "        safety_checker ([`StableDiffusionSafetyChecker`]):\n",
    "            Classification module that estimates whether generated images could be considered offensive or harmful.\n",
    "            Please, refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5) for details.\n",
    "        feature_extractor ([`CLIPFeatureExtractor`]):\n",
    "            Model that extracts features from generated images to be used as inputs for the `safety_checker`.\n",
    "    \"\"\"\n",
    "    _optional_components = [\"safety_checker\", \"feature_extractor\"]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "            self,\n",
    "            prompt: Union[str, List[str]],\n",
    "            height: Optional[int] = None,\n",
    "            width: Optional[int] = None,\n",
    "            num_inference_steps: int = 50,\n",
    "            guidance_scale: float = 7.5,\n",
    "            negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "            num_images_per_prompt: Optional[int] = 1,\n",
    "            eta: float = 0.0,\n",
    "            generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "            latents: Optional[torch.FloatTensor] = None,\n",
    "            output_type: Optional[str] = \"pil\",\n",
    "            return_dict: bool = True,\n",
    "            callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
    "            callback_steps: Optional[int] = 1,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Function invoked when calling the pipeline for generation.\n",
    "\n",
    "        Args:\n",
    "            prompt (`str` or `List[str]`):\n",
    "                The prompt or prompts to guide the image generation.\n",
    "            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
    "                The height in pixels of the generated image.\n",
    "            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
    "                The width in pixels of the generated image.\n",
    "            num_inference_steps (`int`, *optional*, defaults to 50):\n",
    "                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
    "                expense of slower inference.\n",
    "            guidance_scale (`float`, *optional*, defaults to 7.5):\n",
    "                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n",
    "                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n",
    "                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n",
    "                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n",
    "                usually at the expense of lower image quality.\n",
    "            negative_prompt (`str` or `List[str]`, *optional*):\n",
    "                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n",
    "                if `guidance_scale` is less than `1`).\n",
    "            num_images_per_prompt (`int`, *optional*, defaults to 1):\n",
    "                The number of images to generate per prompt.\n",
    "            eta (`float`, *optional*, defaults to 0.0):\n",
    "                Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n",
    "                [`schedulers.DDIMScheduler`], will be ignored for others.\n",
    "            generator (`torch.Generator`, *optional*):\n",
    "                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n",
    "                to make generation deterministic.\n",
    "            latents (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n",
    "                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n",
    "                tensor will ge generated by sampling using the supplied random `generator`.\n",
    "            output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
    "                The output format of the generate image. Choose between\n",
    "                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n",
    "                plain tuple.\n",
    "            callback (`Callable`, *optional*):\n",
    "                A function that will be called every `callback_steps` steps during inference. The function will be\n",
    "                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n",
    "            callback_steps (`int`, *optional*, defaults to 1):\n",
    "                The frequency at which the `callback` function will be called. If not specified, the callback will be\n",
    "                called at every step.\n",
    "\n",
    "        Returns:\n",
    "            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n",
    "            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\n",
    "            When returning a tuple, the first element is a list with the generated images, and the second element is a\n",
    "            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n",
    "            (nsfw) content, according to the `safety_checker`.\n",
    "        \"\"\"\n",
    "\n",
    "        # 0. Default height and width to unet\n",
    "        height = height or self.unet.config.sample_size * self.vae_scale_factor\n",
    "        width = width or self.unet.config.sample_size * self.vae_scale_factor\n",
    "\n",
    "        # 1. Check inputs. Raise error if not correct\n",
    "        self.check_inputs(prompt, height, width, callback_steps)\n",
    "\n",
    "        # 2. Define call parameters\n",
    "        batch_size = 1 if isinstance(prompt, str) else len(prompt)\n",
    "        device = self._execution_device\n",
    "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
    "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
    "        # corresponds to doing no classifier free guidance.\n",
    "        do_classifier_free_guidance = guidance_scale > 1.0\n",
    "\n",
    "        # 3. Encode input prompt\n",
    "        text_embeddings = self._encode_prompt(\n",
    "            prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n",
    "        )\n",
    "\n",
    "        # 4. Prepare timesteps\n",
    "        self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "        timesteps = self.scheduler.timesteps\n",
    "\n",
    "        # 5. Prepare latent variables\n",
    "        try:\n",
    "            num_channels_latents = self.unet.config.in_channels\n",
    "        except Exception or Warning:\n",
    "            num_channels_latents = self.unet.in_channels\n",
    "\n",
    "        latents = self.prepare_latents(\n",
    "            batch_size * num_images_per_prompt,\n",
    "            num_channels_latents,\n",
    "            height,\n",
    "            width,\n",
    "            text_embeddings.dtype,\n",
    "            device,\n",
    "            generator,\n",
    "            latents,\n",
    "        )\n",
    "\n",
    "        # 6. Prepare extra step kwargs. inherit TODO: Logic should ideally just be moved out of the pipeline\n",
    "        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
    "\n",
    "        # 7. Denoising loop\n",
    "        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                # expand the latents if we are doing classifier free guidance\n",
    "                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "                # predict the noise residual\n",
    "                noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "                # perform guidance\n",
    "                if do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "                # compute the previous noisy sample x_t -> x_t-1\n",
    "                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
    "\n",
    "                # call the callback, if provided\n",
    "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
    "                    progress_bar.update()\n",
    "                    if callback is not None and i % callback_steps == 0:\n",
    "                        callback(i, t, latents)\n",
    "\n",
    "        # 8. Post-processing\n",
    "        image = self.decode_latents(latents)\n",
    "\n",
    "        # image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n",
    "        # do_denormalize = [True] * image.shape[0]\n",
    "        # image = self.image_processor.postprocess(image, output_type=output_type, do_denormalize=do_denormalize)\n",
    "\n",
    "        # 9. Run safety checker\n",
    "        has_nsfw_concept = None\n",
    "        # image, has_nsfw_concept = self.run_safety_checker(image, device, text_embeddings.dtype)\n",
    "\n",
    "        # 10. Convert to PIL\n",
    "        if output_type == \"pil\":\n",
    "            image = self.numpy_to_pil(image)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image, has_nsfw_concept)\n",
    "\n",
    "        return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)\n",
    "\n",
    "    def encode_(self, images):\n",
    "        images = (2 * images - 1).clamp(-1.0, 1.0)  # images: [B, 3, H, W]\n",
    "\n",
    "        # encode images\n",
    "        latents = self.vae.encode(images).latent_dist.sample()\n",
    "        latents = self.vae.config.scaling_factor * latents\n",
    "\n",
    "        # scale the initial noise by the standard deviation required by the scheduler\n",
    "        latents = latents * self.scheduler.init_noise_sigma\n",
    "\n",
    "        return latents\n",
    "\n",
    "    def x_augment(self, x: torch.Tensor, img_size: int = 512):\n",
    "        augment_compose = transforms.Compose([\n",
    "            transforms.RandomPerspective(distortion_scale=0.5, p=0.7),\n",
    "            transforms.RandomCrop(size=(img_size, img_size), pad_if_needed=True, padding_mode='reflect')\n",
    "        ])\n",
    "        return augment_compose(x)\n",
    "\n",
    "    def score_distillation_sampling(self,\n",
    "                                    pred_rgb: torch.Tensor,\n",
    "                                    im_size: int,\n",
    "                                    prompt: Union[List, str],\n",
    "                                    negative_prompt: Union[List, str] = None,\n",
    "                                    guidance_scale: float = 100,\n",
    "                                    as_latent: bool = False,\n",
    "                                    grad_scale: float = 1,\n",
    "                                    t_range: Union[List[float], Tuple[float]] = (0.05, 0.95)):\n",
    "        num_train_timesteps = self.scheduler.config.num_train_timesteps\n",
    "        min_step = int(num_train_timesteps * t_range[0])\n",
    "        max_step = int(num_train_timesteps * t_range[1])\n",
    "        alphas = self.scheduler.alphas_cumprod.to(self.device)  # for convenience\n",
    "\n",
    "        # input augmentation\n",
    "        pred_rgb_a = self.x_augment(pred_rgb, im_size)\n",
    "\n",
    "        # the input is intercepted to im_size x im_size and then fed to the vae\n",
    "        if as_latent:\n",
    "            latents = F.interpolate(pred_rgb_a, (64, 64), mode='bilinear', align_corners=False) * 2 - 1\n",
    "        else:\n",
    "            # encode image into latents with vae, requires grad!\n",
    "            latents = self.encode_(pred_rgb_a)\n",
    "\n",
    "        #  Encode input prompt\n",
    "        num_images_per_prompt = 1  # the number of images to generate per prompt\n",
    "        do_classifier_free_guidance = guidance_scale > 1.0\n",
    "        text_embeddings = self._encode_prompt(\n",
    "            prompt, self.device, num_images_per_prompt,\n",
    "            do_classifier_free_guidance,\n",
    "            negative_prompt=negative_prompt,\n",
    "        )\n",
    "\n",
    "        # timestep ~ U(0.05, 0.95) to avoid very high/low noise level\n",
    "        t = torch.randint(min_step, max_step + 1, [1], dtype=torch.long, device=self.device)\n",
    "\n",
    "        # predict the noise residual with unet, stop gradient\n",
    "        with torch.no_grad():\n",
    "            # add noise\n",
    "            noise = torch.randn_like(latents)\n",
    "            latents_noisy = self.scheduler.add_noise(latents, noise, t)\n",
    "            # pred noise\n",
    "            latent_model_input = torch.cat([latents_noisy] * 2) if do_classifier_free_guidance else latents_noisy\n",
    "            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "        # perform guidance (high scale from paper!)\n",
    "        if do_classifier_free_guidance:\n",
    "            noise_pred_uncond, noise_pred_pos = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_pos - noise_pred_uncond)\n",
    "\n",
    "        # w(t), sigma_t^2\n",
    "        w = (1 - alphas[t])\n",
    "        grad = grad_scale * w * (noise_pred - noise)\n",
    "        grad = torch.nan_to_num(grad)\n",
    "\n",
    "        # since we omitted an item in grad, we need to use the custom function to specify the gradient\n",
    "        loss = SpecifyGradient.apply(latents, grad)\n",
    "\n",
    "        return loss, grad.mean()\n",
    "\n",
    "\n",
    "class SpecifyGradient(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    @custom_fwd\n",
    "    def forward(ctx, input_tensor, gt_grad):\n",
    "        ctx.save_for_backward(gt_grad)\n",
    "        # we return a dummy value 1, which will be scaled by amp's scaler so we get the scale in backward.\n",
    "        return torch.ones([1], device=input_tensor.device, dtype=input_tensor.dtype)\n",
    "\n",
    "    @staticmethod\n",
    "    @custom_bwd\n",
    "    def backward(ctx, grad_scale):\n",
    "        gt_grad, = ctx.saved_tensors\n",
    "        gt_grad = gt_grad * grad_scale\n",
    "        return gt_grad, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfe1bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f3d8f05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T05:03:06.029958Z",
     "iopub.status.busy": "2025-04-26T05:03:06.029739Z",
     "iopub.status.idle": "2025-04-26T05:03:06.034101Z",
     "shell.execute_reply": "2025-04-26T05:03:06.033437Z"
    },
    "papermill": {
     "duration": 0.010729,
     "end_time": "2025-04-26T05:03:06.035366",
     "exception": false,
     "start_time": "2025-04-26T05:03:06.024637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def generate_sd_image(pipe: StableDiffusionPipeline, prompt: str, config: SDConfig, num_images_per_prompt: int = 1) -> list[Image.Image]:\n",
    "    all_images = []\n",
    "    for prompt_prefix, prompt_suffix, negative_prompt in zip(config.prompt_prefixes, config.prompt_suffixes, config.negative_prompts):\n",
    "        generator = torch.Generator(device=\"cuda\").manual_seed(0)\n",
    "        images = pipe(\n",
    "            height=512,\n",
    "            width=512,\n",
    "            # prompt=f'a image of {prompt}',\n",
    "            prompt=f'{prompt_prefix} {prompt} {prompt_suffix}',\n",
    "            negative_prompt=negative_prompt,\n",
    "\n",
    "            # prompt=[f\"{prompt}. minimal flat 2d vector icon. lineal color. on a white background. trending on artstation\"],\n",
    "            # negative_prompt=None,\n",
    "\n",
    "            num_inference_steps=config.num_inference_steps,\n",
    "            guidance_scale=config.guidance_scale,\n",
    "            generator=generator,\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "        ).images\n",
    "        all_images += images\n",
    "    return all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02ae0a91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T05:03:06.045135Z",
     "iopub.status.busy": "2025-04-26T05:03:06.044866Z",
     "iopub.status.idle": "2025-04-26T05:03:06.051726Z",
     "shell.execute_reply": "2025-04-26T05:03:06.051151Z"
    },
    "papermill": {
     "duration": 0.013027,
     "end_time": "2025-04-26T05:03:06.052955",
     "exception": false,
     "start_time": "2025-04-26T05:03:06.039928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def generate_primitive_svg(\n",
    "    image: Image.Image,\n",
    "    size: int = 384,\n",
    "    mode: int = 8,\n",
    "    num_shapes: int = 100,\n",
    "    temp_path: str = \"/tmp/image.png\",\n",
    "    max_size: int = 5500\n",
    ") -> str:\n",
    "    temp_path_svg = temp_path.replace(\".png\", \".svg\")\n",
    "    image = image.resize((size, size))\n",
    "\n",
    "    image.save(temp_path)\n",
    "\n",
    "    args = [\n",
    "        \"/tmp/primitive\",\n",
    "        \"-i\",\n",
    "        temp_path,\n",
    "        \"-o\",\n",
    "        temp_path_svg,\n",
    "        \"-n\",\n",
    "        str(num_shapes),\n",
    "        \"-m\",\n",
    "        str(mode),\n",
    "        \"-r\",\n",
    "        f\"{size}\",\n",
    "        \"-s\",\n",
    "        f\"{size}\",\n",
    "    ]\n",
    "    subprocess.run(args)\n",
    "\n",
    "    with open(temp_path_svg, \"r\") as f:\n",
    "        svg = f.read()\n",
    "    \n",
    "    svg = polygon_to_path(svg)\n",
    "    \n",
    "    cur_svg = svg\n",
    "    svg_lines = svg.strip().split(\"\\n\")\n",
    "    keep_idx = [1] * len(svg_lines)\n",
    "    cur_idx = len(svg_lines) - 1\n",
    "\n",
    "    while len(optimize_svg(cur_svg).encode('utf-8')) > max_size:\n",
    "        if \"<path\" in svg_lines[cur_idx]:\n",
    "            keep_idx[cur_idx] = 0\n",
    "\n",
    "        cur_idx -= 1\n",
    "        cur_svg = \"\\n\".join([line for i, line in enumerate(svg_lines) if keep_idx[i]])\n",
    "        \n",
    "    print(f\"Avg keep: {np.mean(keep_idx)}\")\n",
    "    print(len(optimize_svg(cur_svg).encode('utf-8')))\n",
    "    \n",
    "    svg = cur_svg\n",
    "\n",
    "    svg += text_to_svg(\"O\", x_position_frac=0.75, y_position_frac=0.85, font_size=60, color=(255, 255, 255), font_path=\"/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf\").split(\"\\n\")[1]\n",
    "    # svg += text_to_svg(\"O\", x_position_frac=0.6, y_position_frac=0.85, font_size=60, color=(255, 255, 255), font_path=\"/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf\").split(\"\\n\")[1]\n",
    "    # svg += text_to_svg(\"C\", x_position_frac=0.75, y_position_frac=0.85, font_size=60, color=(0, 0, 0), font_path=\"/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf\").split(\"\\n\")[1]\n",
    "\n",
    "    svg = svg.replace(\"</svg>\", \"\") + \"</svg>\"\n",
    "\n",
    "    return svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43c5a449",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T05:03:06.062893Z",
     "iopub.status.busy": "2025-04-26T05:03:06.062667Z",
     "iopub.status.idle": "2025-04-26T05:03:06.080534Z",
     "shell.execute_reply": "2025-04-26T05:03:06.079718Z"
    },
    "papermill": {
     "duration": 0.024056,
     "end_time": "2025-04-26T05:03:06.081670",
     "exception": false,
     "start_time": "2025-04-26T05:03:06.057614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def polygon_to_path(svg: str) -> str:\n",
    "    svg = re.sub(\n",
    "        r'<polygon([\\w\\W]+?)points=([\"\\'])([\\.\\d, -]+?)([\"\\'])', \n",
    "        r'<path\\1d=\\2M\\3z\\4', \n",
    "        svg\n",
    "    )\n",
    "    svg = re.sub(\n",
    "        r'<polyline([\\w\\W]+?)points=([\"\\'])([\\.\\d, -]+?)([\"\\'])', \n",
    "        r'<path\\1d=\\2M\\3\\4', \n",
    "        svg\n",
    "    )\n",
    "    return svg\n",
    "\n",
    "def merge_svgs(bg_svg: str, aest_svg: str):\n",
    "    aest_svg = aest_svg.strip().split(\"\\n\")[2:-1]\n",
    "    # aest_svg = aest_svg.strip().split(\"\\n\")[2:-2]\n",
    "    \n",
    "    # aest_svg = [\n",
    "    #     '<g clip-path=\"polygon(32px 32px, 80px 32px, 80px 80px, 32px 80px)\">',\n",
    "    #     # '<svg x=\"32\" y=\"32\" width=\"48\" height=\"48\" viewBox=\"32 32 48 48\" overflow=\"hidden\">',\n",
    "    #     *aest_svg,\n",
    "    #     '</g>'\n",
    "    # ]\n",
    "    aest_svg = \"\\n\".join(aest_svg)\n",
    "    \n",
    "    bg_svg = polygon_to_path(bg_svg)\n",
    "    svg = bg_svg + '\\n' + aest_svg\n",
    "    svg = svg.replace(\"</svg>\", \"\") + \"</svg>\"\n",
    "\n",
    "    return svg\n",
    "\n",
    "\n",
    "def svg_to_png_no_resize(svg_code: str) -> Image.Image:\n",
    "    png_data = cairosvg.svg2png(bytestring=svg_code.encode('utf-8'))\n",
    "    img_pil = Image.open(io.BytesIO(png_data)).convert('RGB')\n",
    "    return img_pil\n",
    "\n",
    "\n",
    "def apply_random_crop_resize_seed(image: Image.Image, crop_percent=0.05, seed=42):\n",
    "    rs = np.random.RandomState(seed)\n",
    "    \n",
    "    width, height = image.size\n",
    "    crop_pixels_w = int(width * crop_percent)\n",
    "    crop_pixels_h = int(height * crop_percent)\n",
    "\n",
    "    left = rs.randint(0, crop_pixels_w + 1)\n",
    "    top = rs.randint(0, crop_pixels_h + 1)\n",
    "    right = width - rs.randint(0, crop_pixels_w + 1)\n",
    "    bottom = height - rs.randint(0, crop_pixels_h + 1)\n",
    "\n",
    "    image = image.crop((left, top, right, bottom))\n",
    "    image = image.resize((width, height), Image.BILINEAR)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_optimization_settings():\n",
    "    settings = pydiffvg.SvgOptimizationSettings()\n",
    "\n",
    "    lr = 5e-3\n",
    "\n",
    "    settings.global_override([\"optimizer\"], \"Adam\")\n",
    "    settings.global_override([\"color_lr\"], lr)\n",
    "    settings.global_override([\"alpha_lr\"], lr)\n",
    "    settings.global_override([\"paths\", \"shape_lr\"], 10*lr)\n",
    "    settings.global_override([\"circles\", \"shape_lr\"], 10*lr)\n",
    "    settings.global_override([\"transforms\", \"transform_lr\"], 10*lr)\n",
    "    \n",
    "    settings.global_override([\"gradients\", \"optimize_stops\"], True)\n",
    "    settings.global_override([\"gradients\", \"stop_lr\"], lr)\n",
    "    settings.global_override([\"gradients\", \"optimize_color\"], True)\n",
    "    settings.global_override([\"gradients\", \"color_lr\"], lr)\n",
    "    settings.global_override([\"gradients\", \"optimize_alpha\"], True)\n",
    "    settings.global_override([\"gradients\", \"alpha_lr\"], lr)\n",
    "    settings.global_override([\"gradients\", \"optimize_location\"], True)\n",
    "    settings.global_override([\"gradients\", \"location_lr\"], 10*lr)\n",
    "\n",
    "    settings.global_override([\"optimize_color\"], True)\n",
    "    settings.global_override([\"optimize_alpha\"], True)\n",
    "    settings.global_override([\"paths\", \"optimize_points\"], True)\n",
    "    settings.global_override([\"circles\", \"optimize_center\"], True)\n",
    "    settings.global_override([\"circles\", \"optimize_radius\"], True)\n",
    "    settings.global_override([\"transforms\", \"optimize_transforms\"], True)\n",
    "\n",
    "    return settings\n",
    "\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def generate_diffvg_svg(\n",
    "    bg_svg: str,\n",
    "    base_svg_path: str,\n",
    "    evaluator: AestheticEvaluator,\n",
    "    width: int = 384,\n",
    "    height: int = 384,\n",
    "    num_iterations: int = 100,\n",
    "    validation_steps: int = 10,\n",
    "    device: str = device_0,\n",
    "    num_eval: int = 4\n",
    ") -> Image.Image:\n",
    "    pydiffvg.set_use_gpu(True)\n",
    "\n",
    "    bg_image = svg_to_png_no_resize(bg_svg)\n",
    "    bg_image = bg_image.resize((width, height))\n",
    "    bg_image = np.array(bg_image)\n",
    "    bg_image = torch.from_numpy(bg_image).to(device).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "    # mask = torch.zeros((3, height, width), dtype=torch.float32, device=device)\n",
    "    # mask[:, 32:80, 32:80] = 1\n",
    "\n",
    "    settings = get_optimization_settings()\n",
    "\n",
    "    text_path_ids = [f\"text-path-{i}\" for i in range(100)] + [f\"background-{i}\" for i in range(10)]\n",
    "    for text_id in text_path_ids:\n",
    "        text_settings = settings.undefault(text_id)\n",
    "        text_settings[\"paths\"][\"optimize_points\"] = False\n",
    "        text_settings[\"optimize_color\"] = True\n",
    "        text_settings[\"optimize_alpha\"] = True\n",
    "        text_settings[\"optimize_transforms\"] = False\n",
    "\n",
    "    optim_svg = pydiffvg.OptimizableSvg(\n",
    "        base_svg_path, settings, optimize_background=False, verbose=False, device=device\n",
    "    )\n",
    "\n",
    "    best_svg = optimize_svg(merge_svgs(bg_svg, optim_svg.write_xml()))\n",
    "    best_val_loss = -1e8\n",
    "    \n",
    "    grad_accumulation_steps = 1\n",
    "\n",
    "    pbar = tqdm(total=num_iterations)\n",
    "\n",
    "    for iter_idx in range(num_iterations):\n",
    "        optim_svg.zero_grad()\n",
    "        image = optim_svg.render(seed=iter_idx)\n",
    "        img = image[:, :, :3].permute(2, 0, 1).clamp(0, 1)\n",
    "\n",
    "        # img = img * mask + bg_image * (1 - mask)\n",
    "\n",
    "        mask = (img == 0).all(dim=0).unsqueeze(0).float()\n",
    "        img = (1.0 - mask) * img + mask * bg_image\n",
    "        \n",
    "        crop_frac = 0.05\n",
    "        random_size = int(random.uniform(1.0 - crop_frac, 1.0) * image.shape[1])\n",
    "        img = kornia.augmentation.RandomCrop((random_size, random_size))(img.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "        img = apply_preprocessing_torch(img)\n",
    "\n",
    "        loss = aesthetic_score_gradient(evaluator, img).mean()\n",
    "\n",
    "        if iter_idx == 0 or (iter_idx + 1) % validation_steps == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            aest_svg = optim_svg.write_xml()\n",
    "            cur_svg = merge_svgs(bg_svg, aest_svg)\n",
    "            cur_svg_opt = optimize_svg(cur_svg)\n",
    "            pil_image = svg_to_png_no_resize(cur_svg_opt)\n",
    "            val_loss = 0.0\n",
    "            \n",
    "            for eval_idx in range(num_eval):\n",
    "                pil_image_eval = pil_image.copy()\n",
    "                pil_image_eval = apply_random_crop_resize_seed(pil_image_eval, crop_percent=0.03, seed=eval_idx)\n",
    "                pil_image_eval = ImageProcessor(pil_image_eval).apply().image\n",
    "                val_loss += aesthetic_score_original(evaluator, pil_image_eval)\n",
    "\n",
    "            val_loss /= num_eval\n",
    "\n",
    "            # pil_image = Image.fromarray((img_bkp.detach().cpu().permute(1, 2, 0).numpy() * 255).astype(np.uint8)).convert(\"RGB\")\n",
    "\n",
    "            if val_loss > best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_svg = cur_svg_opt\n",
    "            \n",
    "        pbar.set_description(\n",
    "            f\"It {iter_idx}/{num_iterations} | \"\n",
    "            f\"Loss: {loss.item():.3f} | \"\n",
    "            f\"Val Loss: {val_loss:.3f} | \"\n",
    "        )\n",
    "        pbar.update(1)\n",
    "\n",
    "        loss = -loss / grad_accumulation_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if (iter_idx + 1) % grad_accumulation_steps == 0:\n",
    "            optim_svg.step()\n",
    "\n",
    "    print(f\"Best loss: {best_val_loss}\")\n",
    "\n",
    "    return best_svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9fb6238",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T05:03:06.091287Z",
     "iopub.status.busy": "2025-04-26T05:03:06.091021Z",
     "iopub.status.idle": "2025-04-26T05:03:06.095415Z",
     "shell.execute_reply": "2025-04-26T05:03:06.094604Z"
    },
    "papermill": {
     "duration": 0.010466,
     "end_time": "2025-04-26T05:03:06.096564",
     "exception": false,
     "start_time": "2025-04-26T05:03:06.086098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def optimize_svg(svg):\n",
    "    options = scour.scour.parse_args([\n",
    "        '--enable-viewboxing',\n",
    "        '--enable-id-stripping',\n",
    "        '--enable-comment-stripping',\n",
    "        '--shorten-ids',\n",
    "        '--indent=none',\n",
    "        '--strip-xml-prolog',\n",
    "        '--remove-metadata',\n",
    "        '--remove-descriptive-elements',\n",
    "        '--disable-embed-rasters',\n",
    "        '--enable-viewboxing',\n",
    "        '--create-groups',\n",
    "        '--renderer-workaround',\n",
    "        '--set-precision=2',\n",
    "    ])\n",
    "\n",
    "    svg = scour.scour.scourString(svg, options)\n",
    "    \n",
    "    svg = svg.replace('id=\"\"', '')\n",
    "    svg = svg.replace('version=\"1.0\"', '')\n",
    "    svg = svg.replace('version=\"1.1\"', '')\n",
    "    svg = svg.replace('version=\"2.0\"', '')\n",
    "    svg = svg.replace('  ', ' ')\n",
    "    svg = svg.replace('>\\n', '>')\n",
    "    \n",
    "    return svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aaa1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0810d8cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T05:05:14.777558Z",
     "iopub.status.busy": "2025-04-26T05:05:14.777338Z",
     "iopub.status.idle": "2025-04-26T05:05:15.735595Z",
     "shell.execute_reply": "2025-04-26T05:05:15.734903Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.967522,
     "end_time": "2025-04-26T05:05:15.737010",
     "exception": false,
     "start_time": "2025-04-26T05:05:14.769488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import io\n",
    "import math\n",
    "import statistics\n",
    "import string\n",
    "\n",
    "import cairosvg\n",
    "import clip\n",
    "import cv2\n",
    "import kagglehub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from more_itertools import chunked\n",
    "from PIL import Image, ImageFilter\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    PaliGemmaForConditionalGeneration,\n",
    ")\n",
    "\n",
    "svg_constraints = kagglehub.package_import('metric/svg-constraints', bypass_confirmation=True)\n",
    "\n",
    "\n",
    "device_0 = f\"cuda:{max(0, torch.cuda.device_count() - 2)}\"\n",
    "device_1 = device_0\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(\n",
    "    solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, random_seed: int = 0\n",
    ") -> float:\n",
    "    \"\"\"Calculates a fidelity score by comparing generated SVG images to target text descriptions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    solution : pd.DataFrame\n",
    "        A DataFrame containing target questions, choices, and answers about an SVG image.\n",
    "    submission : pd.DataFrame\n",
    "        A DataFrame containing generated SVG strings. Must have a column named 'svg'.\n",
    "    row_id_column_name : str\n",
    "        The name of the column containing row identifiers. This column is removed before scoring.\n",
    "    random_seed : int\n",
    "        A seed to set the random state.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The mean fidelity score (a value between 0 and 1) representing the average similarity between the generated SVGs and their descriptions.\n",
    "        A higher score indicates better fidelity.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ParticipantVisibleError\n",
    "        If the 'svg' column in the submission DataFrame is not of string type or if validation of the SVG fails.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> solution = pd.DataFrame({\n",
    "    ...     'id': [\"abcde\"],\n",
    "    ...     'question': ['[\"Is there a red circle?\", \"What shape is present?\"]'],\n",
    "    ...     'choices': ['[[\"yes\", \"no\"], [\"square\", \"circle\", \"triangle\", \"hexagon\"]]'],\n",
    "    ...     'answer': ['[\"yes\", \"circle\"]'],\n",
    "    ... })\n",
    "    >>> submission = pd.DataFrame({\n",
    "    ...     'id': [\"abcde\"],\n",
    "    ...     'svg': ['<svg viewBox=\"0 0 100 100\"><circle cx=\"50\" cy=\"50\" r=\"40\" fill=\"red\"/></svg>'],\n",
    "    ... })\n",
    "    >>> score(solution, submission, 'row_id', random_seed=42)\n",
    "    0...\n",
    "    \"\"\"\n",
    "    # Convert solution fields to list dtypes and expand\n",
    "    for colname in ['question', 'choices', 'answer']:\n",
    "        solution[colname] = solution[colname].apply(ast.literal_eval)\n",
    "    solution = solution.explode(['question', 'choices', 'answer'])\n",
    "\n",
    "    # Validate\n",
    "    if not pd.api.types.is_string_dtype(submission.loc[:, 'svg']):\n",
    "        raise ParticipantVisibleError('svg must be a string.')\n",
    "\n",
    "    # Check that SVG code meets defined constraints\n",
    "    constraints = svg_constraints.SVGConstraints()\n",
    "    try:\n",
    "        for svg in submission.loc[:, 'svg']:\n",
    "            constraints.validate_svg(svg)\n",
    "    except:\n",
    "        raise ParticipantVisibleError('SVG code violates constraints.')\n",
    "\n",
    "    # Score\n",
    "    vqa_evaluator = VQAEvaluator()\n",
    "    aesthetic_evaluator = AestheticEvaluator()\n",
    "\n",
    "    results = []\n",
    "    all_scores = []\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    try:\n",
    "        df = solution.merge(submission, on='id')\n",
    "        pbar = tqdm(list(set(df[\"id\"])))\n",
    "        for i, (_, group) in enumerate(df.loc[\n",
    "            :, ['id', 'question', 'choices', 'answer', 'svg']\n",
    "        ].groupby('id')):\n",
    "            questions, choices, answers, svg = [\n",
    "                group[col_name].to_list()\n",
    "                for col_name in group.drop('id', axis=1).columns\n",
    "            ]\n",
    "            svg = svg[0]  # unpack singleton from list\n",
    "            group_seed = rng.randint(0, np.iinfo(np.int32).max)\n",
    "            image_processor = ImageProcessor(image=svg_to_png(svg), seed=group_seed).apply()\n",
    "            image = image_processor.image.copy()\n",
    "            aesthetic_score = aesthetic_evaluator.score(image)\n",
    "            vqa_score, batched_choice_probabilities = vqa_evaluator.score(questions, choices, answers, image)\n",
    "            image_processor.reset().apply_random_crop_resize().apply_jpeg_compression(quality=90)\n",
    "            ocr_score = vqa_evaluator.ocr(image_processor.image)\n",
    "            instance_score = (\n",
    "                harmonic_mean(vqa_score, aesthetic_score, beta=0.5) * ocr_score\n",
    "            )\n",
    "            results.append(instance_score)\n",
    "            all_scores.append((instance_score, vqa_score, aesthetic_score, ocr_score, batched_choice_probabilities))\n",
    "            pbar.update(1)\n",
    "\n",
    "    except:\n",
    "        raise ParticipantVisibleError('SVG failed to score.')\n",
    "\n",
    "    fidelity = statistics.mean(results)\n",
    "    return float(fidelity), all_scores\n",
    "\n",
    "\n",
    "\n",
    "class VQAEvaluator:\n",
    "    \"\"\"Evaluates images based on their similarity to a given text description using multiple choice questions.\"\"\"\n",
    "\n",
    "    def __init__(self, model_id: str = 'google/paligemma-2/transformers/paligemma2-10b-mix-448'):\n",
    "        self.quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type='nf4',\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "        self.letters = string.ascii_uppercase\n",
    "        self.model_path = kagglehub.model_download(model_id)\n",
    "        self.processor = AutoProcessor.from_pretrained(self.model_path)\n",
    "        self.model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "            self.model_path,\n",
    "            low_cpu_mem_usage=True,\n",
    "            quantization_config=self.quantization_config,\n",
    "        ).to(device_1)\n",
    "\n",
    "    def score(self, questions, choices, answers, image, n=4):\n",
    "        scores = []\n",
    "        batched_choice_probabilities = []\n",
    "        batches = (chunked(qs, n) for qs in [questions, choices, answers])\n",
    "        for question_batch, choice_batch, answer_batch in zip(*batches, strict=True):\n",
    "            res = self.score_batch(\n",
    "                image,\n",
    "                question_batch,\n",
    "                choice_batch,\n",
    "                answer_batch,\n",
    "            )\n",
    "            scores.extend(res[0])\n",
    "            batched_choice_probabilities.extend(res[1])\n",
    "        return statistics.mean(scores), batched_choice_probabilities\n",
    "\n",
    "    def get_description(self, image: Image.Image, prefix: str = \"<image>cap en\\n\") -> str:\n",
    "        inputs = self.processor(\n",
    "            images=image,\n",
    "            text=prefix,\n",
    "            return_tensors=\"pt\",\n",
    "            # suffix=description,\n",
    "        ).to(\"cuda:0\")\n",
    "        input_len = inputs['input_ids'].shape[-1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
    "\n",
    "        outputs = outputs[0][input_len:]\n",
    "        decoded = self.processor.decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "    def score_batch(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        questions: list[str],\n",
    "        choices_list: list[list[str]],\n",
    "        answers: list[str],\n",
    "    ) -> list[float]:\n",
    "        \"\"\"Evaluates the image based on multiple choice questions and answers.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        image : PIL.Image.Image\n",
    "            The image to evaluate.\n",
    "        questions : list[str]\n",
    "            List of questions about the image.\n",
    "        choices_list : list[list[str]]\n",
    "            List of lists of possible answer choices, corresponding to each question.\n",
    "        answers : list[str]\n",
    "            List of correct answers from the choices, corresponding to each question.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list[float]\n",
    "            List of scores (values between 0 and 1) representing the probability of the correct answer for each question.\n",
    "        \"\"\"\n",
    "        prompts = [\n",
    "            self.format_prompt(question, choices)\n",
    "            for question, choices in zip(questions, choices_list, strict=True)\n",
    "        ]\n",
    "        batched_choice_probabilities = self.get_choice_probability(\n",
    "            image, prompts, choices_list\n",
    "        )\n",
    "\n",
    "        scores = []\n",
    "        for i, _ in enumerate(questions):\n",
    "            choice_probabilities = batched_choice_probabilities[i]\n",
    "            answer = answers[i]\n",
    "            answer_probability = 0.0\n",
    "            for choice, prob in choice_probabilities.items():\n",
    "                if choice == answer:\n",
    "                    answer_probability = prob\n",
    "                    break\n",
    "            scores.append(answer_probability)\n",
    "\n",
    "        # pred_description = self.get_description(image)\n",
    "        batched_choice_probabilities = [{\"question\": questions[i], \"choices\": batched_choice_probabilities[i], \"answer\": answers[i], \"pred_description\": \"\"} for i in range(len(questions))]\n",
    "        return scores, batched_choice_probabilities\n",
    "\n",
    "    def format_prompt(self, question: str, choices: list[str]) -> str:\n",
    "        prompt = f'<image>answer en Question: {question}\\nChoices:\\n'\n",
    "        for i, choice in enumerate(choices):\n",
    "            prompt += f'{self.letters[i]}. {choice}\\n'\n",
    "        return prompt\n",
    "\n",
    "    def mask_choices(self, logits, choices_list):\n",
    "        \"\"\"Masks logits for the first token of each choice letter for each question in the batch.\"\"\"\n",
    "        batch_size = logits.shape[0]\n",
    "        masked_logits = torch.full_like(logits, float('-inf'))\n",
    "\n",
    "        for batch_idx in range(batch_size):\n",
    "            choices = choices_list[batch_idx]\n",
    "            for i in range(len(choices)):\n",
    "                letter_token = self.letters[i]\n",
    "\n",
    "                first_token = self.processor.tokenizer.encode(\n",
    "                    letter_token, add_special_tokens=False\n",
    "                )[0]\n",
    "                first_token_with_space = self.processor.tokenizer.encode(\n",
    "                    ' ' + letter_token, add_special_tokens=False\n",
    "                )[0]\n",
    "\n",
    "                if isinstance(first_token, int):\n",
    "                    masked_logits[batch_idx, first_token] = logits[\n",
    "                        batch_idx, first_token\n",
    "                    ]\n",
    "                if isinstance(first_token_with_space, int):\n",
    "                    masked_logits[batch_idx, first_token_with_space] = logits[\n",
    "                        batch_idx, first_token_with_space\n",
    "                    ]\n",
    "\n",
    "        return masked_logits\n",
    "\n",
    "    def get_choice_probability(self, image, prompts, choices_list) -> list[dict]:\n",
    "        inputs = self.processor(\n",
    "            images=[image] * len(prompts),\n",
    "            text=prompts,\n",
    "            return_tensors='pt',\n",
    "            padding='longest',\n",
    "        ).to(device_0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits[:, -1, :]  # Logits for the last (predicted) token\n",
    "            masked_logits = self.mask_choices(logits, choices_list)\n",
    "            probabilities = torch.softmax(masked_logits, dim=-1)\n",
    "\n",
    "        batched_choice_probabilities = []\n",
    "        for batch_idx in range(len(prompts)):\n",
    "            choice_probabilities = {}\n",
    "            choices = choices_list[batch_idx]\n",
    "            for i, choice in enumerate(choices):\n",
    "                letter_token = self.letters[i]\n",
    "                first_token = self.processor.tokenizer.encode(\n",
    "                    letter_token, add_special_tokens=False\n",
    "                )[0]\n",
    "                first_token_with_space = self.processor.tokenizer.encode(\n",
    "                    ' ' + letter_token, add_special_tokens=False\n",
    "                )[0]\n",
    "\n",
    "                prob = 0.0\n",
    "                if isinstance(first_token, int):\n",
    "                    prob += probabilities[batch_idx, first_token].item()\n",
    "                if isinstance(first_token_with_space, int):\n",
    "                    prob += probabilities[batch_idx, first_token_with_space].item()\n",
    "                choice_probabilities[choice] = prob\n",
    "\n",
    "            # Renormalize probabilities for each question\n",
    "            total_prob = sum(choice_probabilities.values())\n",
    "            if total_prob > 0:\n",
    "                renormalized_probabilities = {\n",
    "                    choice: prob / total_prob\n",
    "                    for choice, prob in choice_probabilities.items()\n",
    "                }\n",
    "            else:\n",
    "                renormalized_probabilities = (\n",
    "                    choice_probabilities  # Avoid division by zero if total_prob is 0\n",
    "                )\n",
    "            batched_choice_probabilities.append(renormalized_probabilities)\n",
    "\n",
    "        return batched_choice_probabilities\n",
    "\n",
    "    def ocr(self, image, free_chars=4):\n",
    "        inputs = (\n",
    "            self.processor(\n",
    "                text='<image>ocr\\n',\n",
    "                images=image,\n",
    "                return_tensors='pt',\n",
    "            )\n",
    "            .to(torch.float16)\n",
    "            .to(self.model.device)\n",
    "        )\n",
    "        input_len = inputs['input_ids'].shape[-1]\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = self.model.generate(**inputs, max_new_tokens=32, do_sample=False)\n",
    "            outputs = outputs[0][input_len:]\n",
    "            decoded = self.processor.decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        num_char = len(decoded)\n",
    "\n",
    "        # Exponentially decreasing towards 0.0 if more than free_chars detected\n",
    "        return min(1.0, math.exp(-num_char + free_chars))\n",
    "\n",
    "\n",
    "class AestheticPredictor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class AestheticEvaluator:\n",
    "    def __init__(self):\n",
    "        # self.model_path = '/kaggle/input/sac-logos-ava1-l14-linearmse/sac+logos+ava1-l14-linearMSE.pth'\n",
    "        # self.clip_model_path = '/kaggle/input/openai-clip-vit-large-patch14/ViT-L-14.pt'\n",
    "        self.model_path = str(kagglehub.model_download(\"jiazhuang/sac-logos-ava1-l14-linearmse/Transformers/default/1\", path=\"sac+logos+ava1-l14-linearMSE.pth\"))\n",
    "        self.clip_model_path = str(kagglehub.model_download(\"jiazhuang/clip-vit-large-patch14/Transformers/default/1\", path=\"ViT-L-14.pt\"))\n",
    "        self.predictor, self.clip_model, self.preprocessor = self.load()\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Loads the aesthetic predictor model and CLIP model.\"\"\"\n",
    "        state_dict = torch.load(self.model_path, weights_only=True, map_location=device_0)\n",
    "\n",
    "        # CLIP embedding dim is 768 for CLIP ViT L 14\n",
    "        predictor = AestheticPredictor(768)\n",
    "        predictor.load_state_dict(state_dict)\n",
    "        predictor.to(device_0)\n",
    "        predictor.eval()\n",
    "        clip_model, preprocessor = clip.load(self.clip_model_path, device=device_0)\n",
    "\n",
    "        return predictor, clip_model, preprocessor\n",
    "\n",
    "    def score(self, image: Image.Image) -> float:\n",
    "        \"\"\"Predicts the CLIP aesthetic score of an image.\"\"\"\n",
    "        image = self.preprocessor(image).unsqueeze(0).to(device_0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = self.clip_model.encode_image(image)\n",
    "            # l2 normalize\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            image_features = image_features.cpu().detach().numpy()\n",
    "\n",
    "        score = self.predictor(torch.from_numpy(image_features).to(device_0).float())\n",
    "\n",
    "        return score.item() / 10.0  # scale to [0, 1]\n",
    "\n",
    "\n",
    "def harmonic_mean(a: float, b: float, beta: float = 1.0) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the harmonic mean of two values, weighted using a beta parameter.\n",
    "\n",
    "    Args:\n",
    "        a: First value (e.g., precision)\n",
    "        b: Second value (e.g., recall)\n",
    "        beta: Weighting parameter\n",
    "\n",
    "    Returns:\n",
    "        Weighted harmonic mean\n",
    "    \"\"\"\n",
    "    # Handle zero values to prevent division by zero\n",
    "    if a <= 0 or b <= 0:\n",
    "        return 0.0\n",
    "    return (1 + beta**2) * (a * b) / (beta**2 * a + b)\n",
    "\n",
    "\n",
    "def svg_to_png(svg_code: str, size: tuple = (384, 384)) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Converts an SVG string to a PNG image using CairoSVG.\n",
    "\n",
    "    If the SVG does not define a `viewBox`, it will add one using the provided size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    svg_code : str\n",
    "        The SVG string to convert.\n",
    "    size : tuple[int, int], default=(384, 384)\n",
    "        The desired size of the output PNG image (width, height).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    PIL.Image.Image\n",
    "        The generated PNG image.\n",
    "    \"\"\"\n",
    "    # Ensure SVG has proper size attributes\n",
    "    if 'viewBox' not in svg_code:\n",
    "        svg_code = svg_code.replace('<svg', f'<svg viewBox=\"0 0 {size[0]} {size[1]}\"')\n",
    "\n",
    "    # Convert SVG to PNG\n",
    "    png_data = cairosvg.svg2png(bytestring=svg_code.encode('utf-8'))\n",
    "    return Image.open(io.BytesIO(png_data)).convert('RGB').resize(size)\n",
    "\n",
    "\n",
    "class ImageProcessor:\n",
    "    def __init__(self, image: Image.Image, seed=None):\n",
    "        \"\"\"Initialize with either a path to an image or a PIL Image object.\"\"\"\n",
    "        self.image = image\n",
    "        self.original_image = self.image.copy()\n",
    "        if seed is not None:\n",
    "            self.rng = np.random.RandomState(seed)\n",
    "        else:\n",
    "            self.rng = np.random\n",
    "\n",
    "    def reset(self):\n",
    "        self.image = self.original_image.copy()\n",
    "        return self\n",
    "    \n",
    "    def visualize_comparison(\n",
    "        self,\n",
    "        original_name='Original',\n",
    "        processed_name='Processed',\n",
    "        figsize=(10, 5),\n",
    "        show=True,\n",
    "    ):\n",
    "        \"\"\"Display original and processed images side by side.\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "        ax1.imshow(np.asarray(self.original_image))\n",
    "        ax1.set_title(original_name)\n",
    "        ax1.axis('off')\n",
    "\n",
    "        ax2.imshow(np.asarray(self.image))\n",
    "        ax2.set_title(processed_name)\n",
    "        ax2.axis('off')\n",
    "\n",
    "        title = f'{original_name} vs {processed_name}'\n",
    "        fig.suptitle(title)\n",
    "        fig.tight_layout()\n",
    "        if show:\n",
    "            plt.show()\n",
    "        return fig\n",
    "\n",
    "    def apply_median_filter(self, size=3):\n",
    "        \"\"\"Apply median filter to remove outlier pixel values.\n",
    "\n",
    "        Args:\n",
    "            size: Size of the median filter window.\n",
    "        \"\"\"\n",
    "        self.image = self.image.filter(ImageFilter.MedianFilter(size=size))\n",
    "        return self\n",
    "\n",
    "    def apply_bilateral_filter(self, d=9, sigma_color=75, sigma_space=75):\n",
    "        \"\"\"Apply bilateral filter to smooth while preserving edges.\n",
    "\n",
    "        Args:\n",
    "            d: Diameter of each pixel neighborhood\n",
    "            sigma_color: Filter sigma in the color space\n",
    "            sigma_space: Filter sigma in the coordinate space\n",
    "        \"\"\"\n",
    "        # Convert PIL Image to numpy array for OpenCV\n",
    "        img_array = np.asarray(self.image)\n",
    "\n",
    "        # Apply bilateral filter\n",
    "        filtered = cv2.bilateralFilter(img_array, d, sigma_color, sigma_space)\n",
    "\n",
    "        # Convert back to PIL Image\n",
    "        self.image = Image.fromarray(filtered)\n",
    "        return self\n",
    "\n",
    "    def apply_fft_low_pass(self, cutoff_frequency=0.5):\n",
    "        \"\"\"Apply low-pass filter in the frequency domain using FFT.\n",
    "\n",
    "        Args:\n",
    "            cutoff_frequency: Normalized cutoff frequency (0-1).\n",
    "                Lower values remove more high frequencies.\n",
    "        \"\"\"\n",
    "        # Convert to numpy array, ensuring float32 for FFT\n",
    "        img_array = np.array(self.image, dtype=np.float32)\n",
    "\n",
    "        # Process each color channel separately\n",
    "        result = np.zeros_like(img_array)\n",
    "        for i in range(3):  # For RGB channels\n",
    "            # Apply FFT\n",
    "            f = np.fft.fft2(img_array[:, :, i])\n",
    "            fshift = np.fft.fftshift(f)\n",
    "\n",
    "            # Create a low-pass filter mask\n",
    "            rows, cols = img_array[:, :, i].shape\n",
    "            crow, ccol = rows // 2, cols // 2\n",
    "            mask = np.zeros((rows, cols), np.float32)\n",
    "            r = int(min(crow, ccol) * cutoff_frequency)\n",
    "            center = [crow, ccol]\n",
    "            x, y = np.ogrid[:rows, :cols]\n",
    "            mask_area = (x - center[0]) ** 2 + (y - center[1]) ** 2 <= r * r\n",
    "            mask[mask_area] = 1\n",
    "\n",
    "            # Apply mask and inverse FFT\n",
    "            fshift_filtered = fshift * mask\n",
    "            f_ishift = np.fft.ifftshift(fshift_filtered)\n",
    "            img_back = np.fft.ifft2(f_ishift)\n",
    "            img_back = np.real(img_back)\n",
    "\n",
    "            result[:, :, i] = img_back\n",
    "\n",
    "        # Clip to 0-255 range and convert to uint8 after processing all channels\n",
    "        result = np.clip(result, 0, 255).astype(np.uint8)\n",
    "\n",
    "        # Convert back to PIL Image\n",
    "        self.image = Image.fromarray(result)\n",
    "        return self\n",
    "\n",
    "    def apply_jpeg_compression(self, quality=85):\n",
    "        \"\"\"Apply JPEG compression.\n",
    "\n",
    "        Args:\n",
    "            quality: JPEG quality (0-95). Lower values increase compression.\n",
    "        \"\"\"\n",
    "        buffer = io.BytesIO()\n",
    "        self.image.save(buffer, format='JPEG', quality=quality)\n",
    "        buffer.seek(0)\n",
    "        self.image = Image.open(buffer)\n",
    "        return self\n",
    "\n",
    "    def apply_random_crop_resize(self, crop_percent=0.05):\n",
    "        \"\"\"Randomly crop and resize back to original dimensions.\n",
    "\n",
    "        Args:\n",
    "            crop_percent: Percentage of image to crop (0-0.4).\n",
    "        \"\"\"\n",
    "        width, height = self.image.size\n",
    "        crop_pixels_w = int(width * crop_percent)\n",
    "        crop_pixels_h = int(height * crop_percent)\n",
    "\n",
    "        left = self.rng.randint(0, crop_pixels_w + 1)\n",
    "        top = self.rng.randint(0, crop_pixels_h + 1)\n",
    "        right = width - self.rng.randint(0, crop_pixels_w + 1)\n",
    "        bottom = height - self.rng.randint(0, crop_pixels_h + 1)\n",
    "\n",
    "        self.image = self.image.crop((left, top, right, bottom))\n",
    "        self.image = self.image.resize((width, height), Image.BILINEAR)\n",
    "        return self\n",
    "\n",
    "    def apply(self):\n",
    "        \"\"\"Apply an ensemble of defenses.\"\"\"\n",
    "        return (\n",
    "            self.apply_random_crop_resize(crop_percent=0.03)\n",
    "            .apply_jpeg_compression(quality=95)\n",
    "            .apply_median_filter(size=9)\n",
    "            .apply_fft_low_pass(cutoff_frequency=0.5)\n",
    "            .apply_bilateral_filter(d=5, sigma_color=75, sigma_space=75)\n",
    "            .apply_jpeg_compression(quality=92)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3384bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2024b560",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T05:03:06.124791Z",
     "iopub.status.busy": "2025-04-26T05:03:06.124585Z",
     "iopub.status.idle": "2025-04-26T05:03:06.130652Z",
     "shell.execute_reply": "2025-04-26T05:03:06.130116Z"
    },
    "papermill": {
     "duration": 0.012139,
     "end_time": "2025-04-26T05:03:06.131789",
     "exception": false,
     "start_time": "2025-04-26T05:03:06.119650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, df=None):\n",
    "        self.df = df\n",
    "\n",
    "        self.device_0 = f\"cuda:{max(0, torch.cuda.device_count() - 2)}\"\n",
    "        self.device_1 = f\"cuda:{max(0, torch.cuda.device_count() - 1)}\"\n",
    "        \n",
    "        self.sd_config = SDConfig()\n",
    "        self.primitive_config = PrimitiveConfig()\n",
    "        self.diffvg_config = DiffvgConfig()\n",
    "        \n",
    "        self.sd_pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            self.sd_config.stable_diffusion_path,\n",
    "            scheduler=DDIMScheduler.from_pretrained(self.sd_config.stable_diffusion_path, subfolder=\"scheduler\"),\n",
    "            torch_dtype=torch.float16,\n",
    "            safety_checker=None\n",
    "        ).to(self.device_1)\n",
    "\n",
    "\n",
    "        # sd_model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "        # self.sd_pipe = LSDSPipeline.from_pretrained(\n",
    "        #     sd_model_id,\n",
    "        #     torch_dtype=torch.float16,\n",
    "        #     # torch_dtype=torch.float32,\n",
    "        #     safety_checker=None,\n",
    "        #     scheduler=PNDMScheduler.from_pretrained(\n",
    "        #         sd_model_id,\n",
    "        #         subfolder=\"scheduler\",\n",
    "        #         local_files_only=False\n",
    "        #     )\n",
    "        # ).to(self.device_1)\n",
    "\n",
    "\n",
    "        self.aesthetic_evaluator = AestheticEvaluator()\n",
    "        self.aesthetic_evaluator.predictor.to(self.device_0).eval()\n",
    "        self.aesthetic_evaluator.clip_model.to(self.device_0).eval()\n",
    "        self.aesthetic_evaluator.predictor.requires_grad_(False)\n",
    "        self.aesthetic_evaluator.clip_model.requires_grad_(False)\n",
    "\n",
    "        self.vqa_evaluator = VQAEvaluator(model_id='google/paligemma-2/transformers/paligemma2-3b-mix-224')\n",
    "        # self.vqa_evaluator = VQAEvaluator(model_id='google/paligemma-2/transformers/paligemma2-10b-mix-448')\n",
    "        self.vqa_evaluator.model.to(self.device_1).eval()\n",
    "        self.vqa_evaluator.model.requires_grad_(False)\n",
    "\n",
    "        with open(self.diffvg_config.base_svg_path, \"r\") as f:\n",
    "            self.base_aest_svg = f.read()\n",
    "        \n",
    "        text_svg = text_to_svg(\"O\", x_position_frac=0.75, y_position_frac=0.85, font_size=60, color=(255, 255, 255), font_path=\"/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf\")\n",
    "        text_img = np.asarray(svg_to_png_no_resize(text_svg), dtype=np.float32)\n",
    "\n",
    "        aest_img = np.asarray(svg_to_png_no_resize(self.base_aest_svg), dtype=np.float32)\n",
    "        self.aest_img = aest_img + text_img\n",
    "\n",
    "        self.mask = (self.aest_img == 0).all(axis=2, keepdims=True).astype(np.float32)\n",
    "        \n",
    "\n",
    "    def score(self, image, prompt):\n",
    "        row = copy.deepcopy(self.df[self.df.description == prompt].iloc[0])\n",
    "        image_processor = ImageProcessor(image=image, seed=0).apply()\n",
    "        image = image_processor.image.copy()\n",
    "        aesthetic_score = self.aesthetic_evaluator.score(image)\n",
    "        question = ast.literal_eval(row.question)\n",
    "        choices = ast.literal_eval(row.choices)\n",
    "        answer = ast.literal_eval(row.answer)\n",
    "        sz = min(len(question), len(choices), len(answer), 4)\n",
    "        assert sz > 0, \"No question, choices, or answer\"\n",
    "        vqa_score, batched_choice_probabilities = self.vqa_evaluator.score(question[:sz], choices[:sz], answer[:sz], image)\n",
    "        image_processor.reset().apply_random_crop_resize().apply_jpeg_compression(quality=90)\n",
    "        ocr_score = self.vqa_evaluator.ocr(image_processor.image)\n",
    "        score = harmonic_mean(vqa_score, aesthetic_score, beta=0.5) * ocr_score\n",
    "        return score, vqa_score, aesthetic_score, ocr_score\n",
    "\n",
    "    def predict(self, prompt: str) -> str:\n",
    "        imgs = generate_sd_image(\n",
    "            pipe=self.sd_pipe,\n",
    "            prompt=prompt,\n",
    "            config=self.sd_config,\n",
    "            num_images_per_prompt=2\n",
    "        )\n",
    "        imgs = [img.resize((384, 384)) for img in imgs]\n",
    "        org_imgs = [copy.deepcopy(img) for img in imgs]\n",
    "        imgs = [np.asarray(img, dtype=np.float32) for img in imgs]\n",
    "        imgs = [img * self.mask + self.aest_img * (1.0 - self.mask) for img in imgs]\n",
    "        imgs = [Image.fromarray(img.astype(np.uint8)) for img in imgs]\n",
    "\n",
    "        # print(len(imgs))\n",
    "        # for ii, img in enumerate(imgs):\n",
    "        #     display(img)\n",
    "        #     # print(self.score(img, prompt))\n",
    "\n",
    "        scores = [self.score(img, prompt) for img in imgs]\n",
    "        img = org_imgs[np.argmax([s[0] for s in scores])]\n",
    "        \n",
    "        print(f\"Scores: {scores}\")\n",
    "        print(f\"Best score: {max([s[0] for s in scores])}\")\n",
    "\n",
    "        bg_svg = generate_primitive_svg(\n",
    "            image=img,\n",
    "            size=self.primitive_config.width,\n",
    "            mode=self.primitive_config.mode,\n",
    "            num_shapes=self.primitive_config.num_shapes,\n",
    "        )\n",
    "        final_svg_opt = optimize_svg(merge_svgs(bg_svg, self.base_aest_svg))\n",
    "        return final_svg_opt\n",
    "        \n",
    "        # final_svg_opt = generate_diffvg_svg(\n",
    "        #     bg_svg=bg_svg,\n",
    "        #     base_svg_path=self.diffvg_config.base_svg_path,\n",
    "        #     evaluator=self.aesthetic_evaluator,\n",
    "        #     width=self.diffvg_config.width,\n",
    "        #     height=self.diffvg_config.height,\n",
    "        #     num_iterations=self.diffvg_config.num_iterations,\n",
    "        #     validation_steps=self.diffvg_config.validation_steps,\n",
    "        #     device=self.device_0\n",
    "        # ) \n",
    "        # # final_svg_opt = optimize_svg(final_svg)\n",
    "\n",
    "        # print(len(final_svg_opt.encode('utf-8')))\n",
    "\n",
    "        # return final_svg_opt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "722d639e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T05:03:06.141646Z",
     "iopub.status.busy": "2025-04-26T05:03:06.141446Z",
     "iopub.status.idle": "2025-04-26T05:05:14.722418Z",
     "shell.execute_reply": "2025-04-26T05:05:14.721568Z"
    },
    "papermill": {
     "duration": 128.58818,
     "end_time": "2025-04-26T05:05:14.724610",
     "exception": false,
     "start_time": "2025-04-26T05:03:06.136430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEST = False\n",
    "\n",
    "if TEST:\n",
    "    model = Model()\n",
    "    \n",
    "    svg = model.predict(\"a blue forest at night\")\n",
    "    \n",
    "    with open(\"output.svg\", \"w\") as f:\n",
    "        f.write(svg)\n",
    "    \n",
    "    image = svg_to_png_no_resize(svg)\n",
    "    \n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b350f1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORG Train\n",
    "drawing_with_llms_path = kagglehub.competition_download('drawing-with-llms')\n",
    "train_df = pd.read_csv(f'{drawing_with_llms_path}/train.csv')\n",
    "\n",
    "train_question_df = pd.read_parquet(f'{drawing_with_llms_path}/questions.parquet')\n",
    "\n",
    "train_df = pd.merge(train_df, train_question_df, how='left', on='id')\n",
    "train_df = train_df.groupby('id').apply(lambda df: df.to_dict(orient='list'), include_groups=False)\n",
    "train_df = train_df.reset_index(name='qa')\n",
    "\n",
    "train_df[\"description\"] = train_df.qa.apply(lambda qa: qa['description'][0])\n",
    "train_df[\"question\"] = train_df.qa.apply(lambda qa: str(json.dumps(qa['question'], ensure_ascii=False)))\n",
    "train_df[\"answer\"] = train_df.qa.apply(lambda qa: str(json.dumps(qa['answer'], ensure_ascii=False)))\n",
    "train_df[\"choices\"] = train_df.qa.apply(lambda qa: str(json.dumps([x.tolist() for x in qa['choices']], ensure_ascii=False)))\n",
    "\n",
    "train_df = train_df.drop(\"qa\", axis=1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89e811b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gen train\n",
    "# train_df = pd.read_parquet(\"/home/mpf/code/kaggle/draw/src/data/generated/qa_dataset_train.parquet\")\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eb0273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edfbebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PUB 100\n",
    "# svg_val_path = kagglehub.dataset_download('raresbarbantan/draw-svg-validation')\n",
    "# val_df = pd.read_csv(f'{svg_val_path}/validation.csv')\n",
    "\n",
    "# val_df = val_df.groupby('id').apply(lambda df: df.to_dict(orient='list'), include_groups=False)\n",
    "# val_df = val_df.reset_index(name='qa')\n",
    "\n",
    "# val_df['description'] = val_df.qa.apply(lambda qa: qa['description'][0])\n",
    "# val_df['question'] = val_df.qa.apply(lambda qa: str(json.dumps(qa['question'], ensure_ascii=False)))\n",
    "# val_df['answer'] = val_df.qa.apply(lambda qa: str(json.dumps(qa['answer'], ensure_ascii=False)))\n",
    "# val_df['choices'] = val_df.qa.apply(lambda qa: str(json.dumps([eval(x) for x in qa['choices']], ensure_ascii=False)))\n",
    "\n",
    "# val_df = val_df.drop(\"qa\", axis=1)\n",
    "\n",
    "# train_df = val_df\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4037520f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304cc679",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(train_df)\n",
    "\n",
    "descriptions = train_df[\"description\"].tolist()\n",
    "svg_list = []\n",
    "\n",
    "for d in tqdm(descriptions):\n",
    "    svg = model.predict(d)\n",
    "    svg_list.append(svg)\n",
    "\n",
    "train_df[\"svg\"] = svg_list\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9c81766",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_parquet(\"train_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca136101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24ee9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "301f86a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616c8d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = train_df[[\"id\", \"question\", \"choices\", \"answer\"]].copy()\n",
    "submission = train_df[[\"id\", \"svg\"]].copy()\n",
    "\n",
    "train_score, all_scores = score(copy.deepcopy(solution), copy.deepcopy(submission), \"row_id\", random_seed=42)\n",
    "print(f\"Scored {train_score: .5f} on {len(submission)} images\")\n",
    "print(train_score)\n",
    "print(np.mean([x[1] for x in all_scores]))\n",
    "print(np.mean([x[2] for x in all_scores]))\n",
    "print(np.mean([x[3] for x in all_scores]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363c6b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85a5400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LB 0.654 - 64 aest\n",
    "# 0.6557589361394732\n",
    "# 0.7138457311515517\n",
    "# 0.507700653076172\n",
    "# 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42b14b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LB 0.68 - 96 aest\n",
    "# 0.7102378486612877\n",
    "# 0.7330874942276718\n",
    "# 0.6678596856043889\n",
    "# 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9687dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LB 0.68 - 96 aest - Best of 4 same prompt - VQA 3b-224\n",
    "# 0.7192526786537395\n",
    "# 0.7448913308011025\n",
    "# 0.6659025150690323\n",
    "# 1.0\n",
    "\n",
    "## LB 0.68 - 96 aest - Best of 2 same prompt - VQA 10b-448\n",
    "# 0.7227565108938497\n",
    "# 0.7464792825711543\n",
    "# 0.6678193693894607\n",
    "# 1.0\n",
    "\n",
    "## LB 0.68 - 96 aest - Best of 2 same prompt - VQA 3b-224\n",
    "# 0.7188254591313346\n",
    "# 0.7436185743609564\n",
    "# 0.6628210720649133\n",
    "# 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df7b1066",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LB * - 144 aest\n",
    "# 0.7056186527098045\n",
    "# 0.7201639261869509\n",
    "# 0.6931755584325547\n",
    "# 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0a4398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LB * - 48 aest\n",
    "# 0.6972650989616372\n",
    "# 0.7483138838098978\n",
    "# 0.579733559290568\n",
    "# 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cdfb4a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T05:05:14.739924Z",
     "iopub.status.busy": "2025-04-26T05:05:14.739695Z",
     "iopub.status.idle": "2025-04-26T05:05:14.742585Z",
     "shell.execute_reply": "2025-04-26T05:05:14.741955Z"
    },
    "papermill": {
     "duration": 0.011706,
     "end_time": "2025-04-26T05:05:14.743760",
     "exception": false,
     "start_time": "2025-04-26T05:05:14.732054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from pathlib import Path\n",
    "# import pandas as pd\n",
    "# import shutil\n",
    "# import torch\n",
    "# import kagglehub\n",
    "# import kaggle_evaluation\n",
    "# import json\n",
    "# import numpy as np\n",
    "\n",
    "# drawing_with_llms_path = kagglehub.competition_download('drawing-with-llms')\n",
    "\n",
    "# comp_path = Path(\"/tmp/drawing-with-llms\")\n",
    "\n",
    "# if not comp_path.exists():\n",
    "#     shutil.copytree(str(drawing_with_llms_path), comp_path)\n",
    "#     shutil.copyfile(comp_path / 'train.csv', comp_path / 'test.csv')\n",
    "\n",
    "# train_df = pd.read_csv(comp_path / 'train.csv')\n",
    "# train_question_df = pd.read_parquet(comp_path / 'questions.parquet')\n",
    "\n",
    "# kaggle_evaluation.test(Model, comp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d9009d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T05:05:14.758589Z",
     "iopub.status.busy": "2025-04-26T05:05:14.758347Z",
     "iopub.status.idle": "2025-04-26T05:05:14.761398Z",
     "shell.execute_reply": "2025-04-26T05:05:14.760598Z"
    },
    "papermill": {
     "duration": 0.011796,
     "end_time": "2025-04-26T05:05:14.762701",
     "exception": false,
     "start_time": "2025-04-26T05:05:14.750905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !cp /tmp/kaggle-evaluation-submission-pb8r6x55.csv pred.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c51db564",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T05:05:15.752219Z",
     "iopub.status.busy": "2025-04-26T05:05:15.751959Z",
     "iopub.status.idle": "2025-04-26T05:05:15.754943Z",
     "shell.execute_reply": "2025-04-26T05:05:15.754334Z"
    },
    "papermill": {
     "duration": 0.01178,
     "end_time": "2025-04-26T05:05:15.756128",
     "exception": false,
     "start_time": "2025-04-26T05:05:15.744348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# train_question_df[\"answer\"] = [json.dumps(x) for x in train_question_df[\"answer\"].values]\n",
    "# train_question_df[\"choices\"] = [json.dumps(x.tolist()) for x in train_question_df[\"choices\"].values]\n",
    "# train_question_df[\"question\"] = [json.dumps(x) for x in train_question_df[\"question\"].values]\n",
    "\n",
    "# pred_df = pd.read_csv(\"./pred.csv\").sort_values(\"id\").reset_index(drop=True)\n",
    "\n",
    "# res = score(\n",
    "#     train_question_df,\n",
    "#     pred_df,\n",
    "#     \"id\"\n",
    "# )\n",
    "\n",
    "# print(res[0])\n",
    "# print(np.mean([x[1] for x in res[1]]))\n",
    "# print(np.mean([x[2] for x in res[1]]))\n",
    "# print(np.mean([x[3] for x in res[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63dc1350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train GEN\n",
    "\n",
    "# 0.6878213772584623\n",
    "# 0.7602925528431133\n",
    "# 0.5227229179480137\n",
    "# 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9ee2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d205b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for description, svg, score in zip(train_df[\"description\"], submission[\"svg\"], all_scores):\n",
    "    img = svg_to_png(svg)\n",
    "    print(description)\n",
    "    print(json.dumps(score, indent=2))\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d4bd9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11735795,
     "sourceId": 89659,
     "sourceType": "competition"
    },
    {
     "datasetId": 6762588,
     "sourceId": 10883959,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7144227,
     "sourceId": 11405535,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7157645,
     "sourceId": 11453951,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7157725,
     "sourceId": 11455128,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 224423433,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 230023480,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 971,
     "modelInstanceId": 3319,
     "sourceId": 4527,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 164716,
     "modelInstanceId": 225001,
     "sourceId": 263093,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 269379,
     "modelInstanceId": 247861,
     "sourceId": 289299,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 269387,
     "modelInstanceId": 247869,
     "sourceId": 289308,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 259.82277,
   "end_time": "2025-04-26T05:05:19.098181",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-26T05:00:59.275411",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01f8cc5a8cd94686ab9c0658285e1414": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "07ef7572d0f844f19de25d5788c9cfed": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "162c22ec373c4d0eaa056c9a49bf8015": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "18ea4fe216984b4292716ed05e2fb304": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1d87267034bc49189229e09f5ce46cc5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "25d4f0811e15474f91282b7fdb6ea12d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2729a252d73040829d04cfc56ba5a350": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "29ccf47f7a2743e6a605a25ed854d351": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c52c6c8f5023408092b4bfc23a7f0827",
       "placeholder": "​",
       "style": "IPY_MODEL_3a370655de894cc08c44bfa2c85b5766",
       "tabbable": null,
       "tooltip": null,
       "value": ""
      }
     },
     "302e6dabd2ed44c8af1a7aeebf863b5c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_29ccf47f7a2743e6a605a25ed854d351",
        "IPY_MODEL_95844d6058854c2db824980765ca2756",
        "IPY_MODEL_767232d4160743cdb29d0373a5c4330c"
       ],
       "layout": "IPY_MODEL_2729a252d73040829d04cfc56ba5a350",
       "tabbable": null,
       "tooltip": null
      }
     },
     "38b5c00b37b64868a84b66757ed4384c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3a370655de894cc08c44bfa2c85b5766": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3accb9d5624246ce93d0a3999b8b462f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3d9c6f4c86d840a9806099e109fa6afd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_162c22ec373c4d0eaa056c9a49bf8015",
       "placeholder": "​",
       "style": "IPY_MODEL_18ea4fe216984b4292716ed05e2fb304",
       "tabbable": null,
       "tooltip": null,
       "value": " 40/40 [00:23&lt;00:00,  1.44it/s]"
      }
     },
     "445a95741d6d4267baf6738a185e2bc3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "45a498fd303b4435992218b921c19e73": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_55ca910de2614bfdb68af7149f85319a",
        "IPY_MODEL_5955bde0473440689224076af35b3f5e",
        "IPY_MODEL_55f4b50f55c541ed894be674ec42a64e"
       ],
       "layout": "IPY_MODEL_aace0a722f2a4bda8b454122fa008297",
       "tabbable": null,
       "tooltip": null
      }
     },
     "55ca910de2614bfdb68af7149f85319a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ee1b0cb878d94ec9b011a775a0471ca5",
       "placeholder": "​",
       "style": "IPY_MODEL_d90dc5dce1b64c49a922b0fdfc505170",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "55f4b50f55c541ed894be674ec42a64e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b01e519e0cc74567a95720cfba81bf89",
       "placeholder": "​",
       "style": "IPY_MODEL_dafe3931f23a4c55b6df0e9ab8367734",
       "tabbable": null,
       "tooltip": null,
       "value": " 50/50 [00:18&lt;00:00,  2.77it/s]"
      }
     },
     "58e823a4266f41eb8f43b56704ce4081": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3accb9d5624246ce93d0a3999b8b462f",
       "max": 6,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e6a7c6f650da4d34bb552aa886f129a9",
       "tabbable": null,
       "tooltip": null,
       "value": 6
      }
     },
     "5955bde0473440689224076af35b3f5e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_07ef7572d0f844f19de25d5788c9cfed",
       "max": 50,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e35509aca34244b99b49e1b70c37f81e",
       "tabbable": null,
       "tooltip": null,
       "value": 50
      }
     },
     "596916d769ba450095f5a7010a3a8579": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5ee19437f7a640ddb3f2c31d1e50ba64": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "767232d4160743cdb29d0373a5c4330c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_25d4f0811e15474f91282b7fdb6ea12d",
       "placeholder": "​",
       "style": "IPY_MODEL_b6fdddb38b3340fda98887ce151b6d6e",
       "tabbable": null,
       "tooltip": null,
       "value": " 0/0 [00:00&lt;?, ?it/s]"
      }
     },
     "805fcca2aa9c4285b0e3a9d0743c1330": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1d87267034bc49189229e09f5ce46cc5",
       "placeholder": "​",
       "style": "IPY_MODEL_a4cbf4ed200d4ffda208663fc08dc34b",
       "tabbable": null,
       "tooltip": null,
       "value": " 6/6 [00:30&lt;00:00,  4.10s/it]"
      }
     },
     "95844d6058854c2db824980765ca2756": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9dc4e0e19eb94844bc5e181172c302d0",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_01f8cc5a8cd94686ab9c0658285e1414",
       "tabbable": null,
       "tooltip": null,
       "value": 0
      }
     },
     "96b0148cafb24b7384b889e413de1802": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9dc4e0e19eb94844bc5e181172c302d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "9fc73f21f58f42ac8511c33ef378adee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5ee19437f7a640ddb3f2c31d1e50ba64",
       "max": 40,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_445a95741d6d4267baf6738a185e2bc3",
       "tabbable": null,
       "tooltip": null,
       "value": 40
      }
     },
     "a4cbf4ed200d4ffda208663fc08dc34b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "aace0a722f2a4bda8b454122fa008297": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aba0c44346544e78b6fce4951fb7e4f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b01e519e0cc74567a95720cfba81bf89": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b6fdddb38b3340fda98887ce151b6d6e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ba6e7844e30a4e029bbcf73899a2d1e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c1ddf721bc4c419b95ec9e8dd0e40d9c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e1481b539a3f4f28ae8ceb8bc1b208fe",
        "IPY_MODEL_9fc73f21f58f42ac8511c33ef378adee",
        "IPY_MODEL_3d9c6f4c86d840a9806099e109fa6afd"
       ],
       "layout": "IPY_MODEL_ce675e3bfe884275b8b5a3cde8ebbead",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c52c6c8f5023408092b4bfc23a7f0827": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ce675e3bfe884275b8b5a3cde8ebbead": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d90dc5dce1b64c49a922b0fdfc505170": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "dafe3931f23a4c55b6df0e9ab8367734": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "db2ceebe9fa14e83b80409af1ee19be9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_96b0148cafb24b7384b889e413de1802",
       "placeholder": "​",
       "style": "IPY_MODEL_ba6e7844e30a4e029bbcf73899a2d1e0",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading pipeline components...: 100%"
      }
     },
     "e1481b539a3f4f28ae8ceb8bc1b208fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_aba0c44346544e78b6fce4951fb7e4f4",
       "placeholder": "​",
       "style": "IPY_MODEL_596916d769ba450095f5a7010a3a8579",
       "tabbable": null,
       "tooltip": null,
       "value": "It 39/40 | Loss: 0.562 | Val Loss: 0.561 | : 100%"
      }
     },
     "e35509aca34244b99b49e1b70c37f81e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e6a7c6f650da4d34bb552aa886f129a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ee1b0cb878d94ec9b011a775a0471ca5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f9014ae63bd54f689c42927ee1953bc1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_db2ceebe9fa14e83b80409af1ee19be9",
        "IPY_MODEL_58e823a4266f41eb8f43b56704ce4081",
        "IPY_MODEL_805fcca2aa9c4285b0e3a9d0743c1330"
       ],
       "layout": "IPY_MODEL_38b5c00b37b64868a84b66757ed4384c",
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
